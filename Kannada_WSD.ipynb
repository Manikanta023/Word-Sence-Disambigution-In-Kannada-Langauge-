{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tkinter as tk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('Dataset.json', 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Extract the necessary information from the dataset\n",
    "    data = []\n",
    "    for item in dataset:\n",
    "        word = item['word']\n",
    "        translation = item['translation']\n",
    "        senses = item['senses']\n",
    "        meanings = [item['disambiguation']]\n",
    "        data.append({'Word': word, 'Translation': translation, 'Sense': senses, 'Meaning': '; '.join(meanings)})\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Zà²€-\\u25FF\\u2600-\\u26FF\\u2700-\\u27BF]', ' ', text)\n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "def extract_features(word):\n",
    "    # Example feature extraction: Part-of-speech tagging\n",
    "    pos_tags = pos_tag([word])\n",
    "    features = {}\n",
    "    if len(pos_tags) > 0:\n",
    "        features['POS'] = pos_tags[0][1]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Disambiguation\n",
    "def rule_based_disambiguate_word(word, dataset, features):\n",
    "    # Iterate through the dataset and check for word matches\n",
    "    for index, row in dataset.iterrows():\n",
    "        if row['Word'] == word or row['Translation'] == word:\n",
    "            # Check if all the features match\n",
    "            if all(feature in row for feature in features.values()):\n",
    "                return row['Meaning']\n",
    "\n",
    "    # If no match is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_model(dataset):\n",
    "    # Preprocess the dataset\n",
    "    dataset['Preprocessed'] = dataset['Meaning'].apply(preprocess_text)\n",
    "\n",
    "    # Extract features using TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(dataset['Preprocessed'])\n",
    "    y = dataset['Sense']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the LinearSVC model\n",
    "    model = LinearSVC()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate accuracy on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "def predict_sense(text, dataset, model, vectorizer):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    words = preprocessed_text.split()\n",
    "    word_senses = {}\n",
    "    ambiguous_words = {}\n",
    "\n",
    "    for word in words:\n",
    "        features = extract_features(word)\n",
    "        sense = rule_based_disambiguate_word(word, dataset, features)\n",
    "        if sense is None:\n",
    "            X = vectorizer.transform([preprocessed_text])\n",
    "            predicted_senses = model.predict(X)\n",
    "\n",
    "            word_senses[word] = predicted_senses[0]\n",
    "            if word not in ambiguous_words:\n",
    "                ambiguous_words[word] = {'count': 0, 'meanings': []}\n",
    "            word_meanings = dataset.loc[(dataset['Word'] == word) & (dataset['Sense'].isin(predicted_senses)), 'Meaning'].unique()\n",
    "            if len(word_meanings) > 0:\n",
    "                ambiguous_words[word]['count'] += 1\n",
    "                ambiguous_words[word]['meanings'].extend(list(word_meanings))\n",
    "\n",
    "    # Remove words from ambiguous_words that have count = 0\n",
    "    ambiguous_words = {word: data for word, data in ambiguous_words.items() if data['count'] > 0}\n",
    "\n",
    "    # Remove word_senses for words not in the dataset\n",
    "    word_senses = {word: sense for word, sense in word_senses.items() if word in dataset['Word'].values}\n",
    "\n",
    "    return word_senses, ambiguous_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Interface Functions\n",
    "def disambiguate(output_text):\n",
    "    global input_text, dataset, model, vectorizer\n",
    "\n",
    "    text = input_text.get(\"1.0\", \"end\").strip()\n",
    "    if text:\n",
    "        word_senses, ambiguous_words = predict_sense(text, dataset, model, vectorizer)\n",
    "        ambiguous_words_count = 0\n",
    "        ambiguous_words_list = []\n",
    "\n",
    "        for word, data in ambiguous_words.items():\n",
    "            count = data['count']\n",
    "            word_meanings = data['meanings']\n",
    "\n",
    "            # Check if the word has any valid meanings in the dataset\n",
    "            valid_meanings = [meaning for meaning in word_meanings if meaning != 'Meaning not found']\n",
    "            if valid_meanings:\n",
    "                ambiguous_words_count += 1\n",
    "                ambiguous_words_list.append(word)\n",
    "\n",
    "        # Display ambiguous word senses count\n",
    "        output_text.insert(tk.END, f\"Ambiguous words count: {ambiguous_words_count}\\n\\n\")\n",
    "\n",
    "        # Display ambiguous word senses and their meanings\n",
    "        for word in ambiguous_words_list:\n",
    "            data = ambiguous_words[word]\n",
    "            count = data['count']\n",
    "            word_meanings = data['meanings']\n",
    "            output_text.insert(tk.END, f\"Word: {word}\\n\")\n",
    "            # output_text.insert(tk.END, f\"Senses count: {count}\\n\")\n",
    "            output_text.insert(tk.END, \"Meanings:\\n\")\n",
    "            output_text.insert(tk.END, \"\\n\".join([f\"{i+1}. {meaning}\" for i, meaning in enumerate(word_meanings)]))\n",
    "            output_text.insert(tk.END, \"\\n\\n\")\n",
    "\n",
    "        if ambiguous_words_count == 0:\n",
    "            output_text.insert(tk.END, \"No ambiguous words found\\n\\n\")\n",
    "\n",
    "        # Print disambiguated meanings\n",
    "        for word, sense in word_senses.items():\n",
    "            if word not in ambiguous_words_list:\n",
    "                meanings = dataset.loc[dataset['Word'] == word, 'Meaning'].values\n",
    "                output_text.insert(tk.END, f\"Word: {word}\\n\")\n",
    "                # output_text.insert(tk.END, f\"Senses count: {sense}\\n\")\n",
    "                output_text.insert(tk.END, \"Meanings:\\n\")\n",
    "                output_text.insert(tk.END, \"\\n\".join([f\"{i+1}. {meaning}\" for i, meaning in enumerate(meanings)]))\n",
    "                output_text.insert(tk.END, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(dataset, model, vectorizer, output_text):\n",
    "    # Prepare the test data\n",
    "    test_data = dataset.sample(frac=0.2, random_state=42)  # Use 20% of the dataset for testing\n",
    "    test_X = vectorizer.transform(test_data['Preprocessed'])\n",
    "    test_y = test_data['Sense']\n",
    "\n",
    "    # Predict the senses\n",
    "    predicted_y = model.predict(test_X)\n",
    "\n",
    "    # Calculate and display the classification report\n",
    "    report = classification_report(test_y, predicted_y)\n",
    "    output_text.insert(tk.END, \"Classification Report:\\n\")\n",
    "    output_text.insert(tk.END, report)\n",
    "    output_text.insert(tk.END, \"\\n\")\n",
    "\n",
    "    # Calculate and display the accuracy\n",
    "    accuracy = accuracy_score(test_y, predicted_y)\n",
    "    output_text.insert(tk.END, f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "def main():\n",
    "    global input_text, dataset, model, vectorizer, output_text\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset()\n",
    "\n",
    "    # Train the machine learning model\n",
    "    model, vectorizer = train_model(dataset)\n",
    "\n",
    "    # Measure the performance\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Word Sense Disambiguation\")\n",
    "    root.geometry(\"400x400\")\n",
    "\n",
    "    output_text = tk.Text(root, height=15)\n",
    "    output_text.pack()\n",
    "\n",
    "    measure_performance(dataset, model, vectorizer, output_text)\n",
    "\n",
    "    label = tk.Label(root, text=\"Enter a sentence:\")\n",
    "    label.pack()\n",
    "\n",
    "    input_text = tk.Text(root, height=5)\n",
    "    input_text.pack()\n",
    "\n",
    "    button = tk.Button(root, text=\"Disambiguate\", command=lambda: disambiguate(output_text))\n",
    "    button.pack()\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
